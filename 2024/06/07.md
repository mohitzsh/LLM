# [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

I'll use LLaMA paper as an entry point into the world of large language models. This is the first pass, so I'll skip lots of details and rather collect pointers to other papers LLaMA builds upon to read in the future.


# Summary
* Trains models from 7B to 65B parameter size to optimize for inference budget using publicly available training datasets.
* LLaMA-13B similar to GPT-3 (175B) and LLaMA-65B to PaLM-540B and Chinchilla-70B in performance.
* Key is to train smaller parameters on more tokens.

# Background
* Few-shot learning capabilities for LLMs reported in [GPT-3](https://arxiv.org/abs/2005.14165) paper. Scaling laws were studied in [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) paper. TODO: Relationship between GPT-3 and LLaMA paper. Some papers that discusses art of model scaling are PaLM, and Gopher.
* Chinchilla paper brings training compute budget into the equation - given a fixed training compute budget, what's the recipe to train the best model. Do we scale the parameters, or the data. The verdict was to train smaller models for much longer.
* LLaMA paper brings inference budget into the equation and finds that training smaller models with much more tokens is the way to go. While existing papers were training on billions of tokens (order of 100s of B), LLaMA proposes to train over 1T tokens.
* Another selling point of LLaMA paper is to train only on publicly available data. Other notable models that follow this strrategy are: OPT, GPT-NeoX, BLOOM, GLM but aren't competitive with the largest models like PaLM-62B or Chinchilla.

# Pre-training Data
* **Common Crawl**: Take temporal dumps of common-crawl dataset and apply pre-processing on it to find high quality english dataset. This is based on CCNet pipeline to prepare english text pre-training data. Data preparation is a topic for itself deserving several posts.
* **C4**: Colossus Cleaned Crawl Dataset (C4) is a preprocessed version of common crawl dataset.
* **Github**
* **Wikipedia**: Interesting thing about wikipedia dataset is that they chose languages other than english that are written in Latin or Cyrillic script.
* Gutenbern and Books3 dataset for book kind of data.
* **ArXiv**: For scientific literature ranging different domains. Preprocessing is interesting since they work on tex files so macros need to be normalized.
* **Stack Exchange**: Question answering flavors.

# Modeling Details
* Used standard BPE based tokenizer from Sentence-Piece paper. 1.4T tokens in training dataset, only seen once during training with some exception (Wikipedia and Books are seen twice). TODO: How do you ensure a fixed number of epochs for certain tokens? Are datasets fed to the model in some sequence? So when it's the turn of Wikipedia dataset, you can do two epochs.
* Normalization is done on the input side of transformer instead of output. RMSNorm is used.
* SwiGLU non-linear units.
* Rotary Embeddings (RoPE) at each layer of network.

| params | dimensions | n heads | n layers | learning rate | batch size | n tokens |
| --- | --- | --- | --- | --- | --- | --- |
| 6.7B | 4096 | 32 | 32 | 3e-4 | 4M | 1T |
| 13B | 5120 | 40 | 40 | 3e-4 | 4M | 1T |
| 32.5B | 6656 | 52 | 60 | 1.5e-4 | 4M | 1.4T |
| 65.2B | 8192 | 64 | 80 | 1.5e-4 | 4M | 1.4T |

* Interesting how they are able to train with 4M batch size. TODO: Do you need to scale the learning rate with batch size?
* Higher learning rate for more tokens sounds intuitive.
* Cosine learning rate schedule, some warmup in the learning rate.

# Efficient Implementation of model
* Use flashattention for efficient attention implementation. This is available in xformer library.
* Several techniques for Model parallelism & sequence parallelism are discussed in [this](https://arxiv.org/pdf/2205.05198) that allow efficient training of transformer models.

# Next Up
1. [GPT-3] [Language models are few shot learners](https://arxiv.org/abs/2005.14165)
2. [Scaling Laws for Natural Language Models](https://arxiv.org/abs/2001.08361)
3. [PaLM] [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
4. [Gopher][Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
5. [Chinchilla] [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
6. [OPT] [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
7. [GPT-NeoX] [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
8. [BLOOM] [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
9. [GLM] [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)
10. [CCNet] [CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data](https://arxiv.org/abs/1911.00359)
11. [Fineweb] [Fineweb: 15 trillion tokens of the finest data the üåê web has to offer](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
12. [C4] [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)
13. [BPE] [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
14. [SentencePiece] [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226)
15. [RMSNorm] [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)
16. [SwiGLU] [GLU Variants improve transformers](https://arxiv.org/pdf/2002.05202)
17. [AdamW] [Decoupled weight decay regularization](https://arxiv.org/abs/1711.05101)
18. [FlashAttention] [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
19. [Self-attention Does Not Need O(n2) Memory](https://arxiv.org/abs/2112.05682)
20. [Reducing Activeation Recomputation in Large Language Models](https://arxiv.org/abs/2205.05198)
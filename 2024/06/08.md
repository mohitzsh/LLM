# Improving Language Understanding by Generative Pre-Training (GPT1)

# Background
* Introduce semi-supervised training scheme for NLP tasks: unsupervised language modeling pre-training (on any text corpus, BooksCorpus used for in this paper), and task specific finetuning on target task.
* Multiple tasks supported for finetuning by adopting task specific input representation compatible with how pretraining is done (representing input simply as a series of tokens).
* Finetuning accompanied by using combination of task specific and language modeling objective function (appropriately weighted)

# Model
* Decoder only transformer, BPE encoding, GELU non-linearaity, learned positional embedding, learning rate: warmup, annealed back to zero using cosine schedule.
* Interestingly, batch sizes are still reasonable (64 for pre-training and 32 for finetuning). Probably because they don't need to parallelize as aggressivly as LLMs need to be. The pre-training dataset is much smaller compared to what LLMs like LLaMA is trained on.


# Up Next
1. [Generating Wikipedia by summarizing long sequences](https://arxiv.org/abs/1801.10198)
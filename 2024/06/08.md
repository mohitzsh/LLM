# Improving Language Understanding by Generative Pre-Training (GPT1)

# Background
* Introduce semi-supervised training scheme for NLP tasks: unsupervised language modeling pre-training (on any text corpus, BooksCorpus used for in this paper), and task specific finetuning on target task.
* Multiple tasks supported for finetuning by 


# Up Next
1. [Reasoning about Entailment with Neural Attention](https://arxiv.org/abs/1509.06664)